{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Critical Points with TensorFlow\n",
    "## Part 1b - Critical Points of Quadratic Forms - Gradient Norm Minimzation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative, possibly more intuitive, method of finding critical points\n",
    "is to simply minimize the norm of the gradient directly.\n",
    "\n",
    "That is, we are interested in points where the gradient is close to the $0$ vector.\n",
    "To find these points,\n",
    "we descend a new function $g$\n",
    "that is defined in terms of the\n",
    "gradients of our original function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "g(\\theta) = \\frac{1}{2}\\|\\nabla f(\\theta) \\|_2^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick application of the chain rule gives the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "\\nabla g(\\theta) &= \\nabla\\|\\nabla f(\\theta) \\|_2^2\\\\\n",
    "&= \\nabla\\nabla f(\\theta)\\cdot \\nabla f(\\theta)\\\\\n",
    "&= \\nabla^2f(\\theta)\\nabla f(\\theta)\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "leading to the update rule:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "\\theta^{t+1} &= \\theta^{t} - \\eta \\nabla g(\\theta)\\\\\n",
    "&= \\theta^{t} - \\eta\\nabla^2f(\\theta)\\nabla f(\\theta)\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare this to the Newton update:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "\\theta^{t+1} &= \\theta^{t} - \\gamma \\left(\\nabla^2f(\\theta)\\right)^{-1}\\nabla f(\\theta)\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the intuitive appeal of the former method,\n",
    "an analysis of its performance on polynomial functions\n",
    "would seem to indicate that it is a *bad idea*,\n",
    "because it converges more slowly when the step size is correctly chosen\n",
    "and diverges horribly if it is not.\n",
    "\n",
    "It is unclear, however, whether this carries over to other kinds of functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import crit_finder\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing with Identity Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The identity matrix quadratic form also makes a sanity check for the gradient norm minimization technique,\n",
    "since the Hessian is the identity,\n",
    "and so the updates from gradient norm minimization should exactly match those\n",
    "from gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "N = 2\n",
    "\n",
    "identity_matrix = np.eye(N).astype(np.float32)\n",
    "\n",
    "initial_values = np.random.standard_normal(size=N).astype(np.float32)\n",
    "\n",
    "identity_quadratic_form = crit_finder.make_quadratic_form(identity_matrix, initial_values,\n",
    "                                                                 crit_finder.DEFAULTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.9315233e-05, array([0.00741813, 0.00189783], dtype=float32))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradmin_final_output, gradmin_final_parameters = crit_finder.minimize(identity_quadratic_form, \"gradient_norm_min\", 50)\n",
    "gradmin_final_output, gradmin_final_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.9315233e-05, array([0.00741813, 0.00189783], dtype=float32))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gd_final_output, gd_final_parameters = crit_finder.minimize(identity_quadratic_form, \"gradient_descent\", 50)\n",
    "\n",
    "gd_final_output, gd_final_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all(np.equal(gradmin_final_parameters, gd_final_parameters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Symmetric Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We again first extend to the case of random symmetric matrices\n",
    "(for more on the specific random ensemble, see\n",
    "[the previous notebook](./01a - Critical Points of Quadratic Forms - Newton's Method.ipynb))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gaussian(N):\n",
    "    return 1/np.sqrt(N)*np.random.standard_normal(size=(N,N)).astype(np.float32)\n",
    "\n",
    "def generate_symmetric(N):\n",
    "    base_matrix = generate_gaussian(N)\n",
    "    return (1/np.sqrt(2))*(base_matrix+base_matrix.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.327518e-05"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 5\n",
    "\n",
    "initial_values = 1/np.sqrt(N)*np.random.standard_normal(size=N).astype(np.float32)\n",
    "\n",
    "random_symmetric_matrix = generate_symmetric(N)\n",
    "\n",
    "random_symmetric_quadratic_form = crit_finder.make_quadratic_form(random_symmetric_matrix, initial_values,\n",
    "                                                                 crit_finder.DEFAULTS)\n",
    "\n",
    "_, values = crit_finder.minimize(random_symmetric_quadratic_form, \"gradient_norm_min\", 5000)\n",
    "\n",
    "np.linalg.norm(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite also using curvature information,\n",
    "gradient norm minimization takes *far* more steps to reach a given error than\n",
    "does Newton's method:\n",
    "while Newton can get to solutions with norm of order `1e-8` in two or three steps,\n",
    "gradient norm minimization takes thousands of steps even on small problems,\n",
    "with the number of steps increasing with problem size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Positive Definite Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The advantage of gradient norm minimization,\n",
    "however, is that it does not require a matrix inverse calculation.\n",
    "\n",
    "This has a computational benefit,\n",
    "since the matrix inversion step is complexity $O(n^{k})$\n",
    "for some $k$ in $(2, 3]$,\n",
    "depending on the algorithm,\n",
    "while all other computational steps are at most complexity $O(n^2\\log n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But even more crucially,\n",
    "avoiding the matrix inverse means that all of the issues regarding numerical non-invertibility\n",
    "that bedeviled the Newton's methods discussed in the last notebook\n",
    "are avoided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We again select random ill-conditioned matrices\n",
    "according to the Wishart distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_wishart(N):\n",
    "    self_outer_product = lambda x: x.dot(x.T)\n",
    "    wishart_random_matrix = 1/N*self_outer_product(np.random.standard_normal(size=(N,1))).astype(np.float32)\n",
    "    \n",
    "    return wishart_random_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 500\n",
    "\n",
    "wishart_random_matrix = generate_wishart(N)\n",
    "\n",
    "initial_values = 1/np.sqrt(N)*np.random.standard_normal(size=N).astype(np.float32)\n",
    "\n",
    "wishart_quadratic_form = crit_finder.make_quadratic_form(wishart_random_matrix, initial_values,\n",
    "                                                                 crit_finder.DEFAULTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, we discover that gradient descent and gradient norm minimization\n",
    "end up at almost exactly the same (wrong) solution!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.5077608e-07"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, gd_solution = crit_finder.minimize(wishart_quadratic_form, \"gradient_descent\", 500)\n",
    "_, gradient_norm_min_solution = crit_finder.minimize(wishart_quadratic_form, \"gradient_norm_min\", 500)\n",
    "\n",
    "np.linalg.norm(gd_solution-gradient_norm_min_solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How could this be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a quadratic form\n",
    "$$f(x) = \\frac{1}{2}\\theta^\\intercal Q \\theta$$\n",
    "denote by $M$ the Hessian matrix\n",
    "$$\\nabla^2 f(\\theta) = \\frac{1}{2}(Q+Q^\\intercal)\\ \\colon=M$$\n",
    "and recall that the gradient is\n",
    "$$\\nabla f(\\theta) = M\\theta$$\n",
    "\n",
    "Assuming WLOG that the step size is $1$,\n",
    "the gradient descent update is\n",
    "$$\n",
    "\\theta^{t+1} = \\theta^{t} - \\nabla f(\\theta) = \\theta^t - M\\theta\n",
    "$$\n",
    "\n",
    "while the gradient norm minimization update is\n",
    "\n",
    "$$\n",
    "\\theta^{t+1} = \\theta^{t} - \\nabla^2 f(\\theta) \\nabla f(\\theta) = \\theta^{t} - MM\\theta$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And so for the updates to be approximately equal we need to have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "MM\\theta \\approx M\\theta \\ \\ \\ \\forall \\theta \\\\\n",
    "\\therefore M^2 \\approx M\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The condition of exact equality there is called\n",
    "[idempotence](https://en.wikipedia.org/wiki/Idempotent_matrix).\n",
    "A matrix is idempotent if\n",
    "(and perhaps only if, but I haven't rigorously proved this)\n",
    "its eigenvalues are all $0$ or $1$.\n",
    "\n",
    "This is almost surely not exactly the case for our matrices,\n",
    "but then again,\n",
    "we saw that the updates weren't exactly equal.\n",
    "Let's take a look at the spectrum of $M$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = wishart_random_matrix\n",
    "\n",
    "M = 0.5*(Q+Q.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7feafc362630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(np.linalg.eig(M)[0], normed=True, bins=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, the eigenvalues are all close to $0$ or $1$.\n",
    "\n",
    "We can also check out conditions directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.019451223"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(M.dot(M)-M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0015909893"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(M.dot(M).dot(initial_values) - M.dot(initial_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And so not only does gradient norm minimization use more computational effort than gradient descent,\n",
    "it does so while also making almost the exact same updates as gradient descent for certain problems!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
