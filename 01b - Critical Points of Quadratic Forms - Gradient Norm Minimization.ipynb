{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Critical Points with TensorFlow\n",
    "## Part 1b - Critical Points of Quadratic Forms - Gradient Norm Minimzation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative, possibly more intuitive, method of finding critical points\n",
    "is to simply minimize the norm of the gradient directly.\n",
    "\n",
    "That is, we are interested in points where the gradient is close to the $0$ vector.\n",
    "To find these points,\n",
    "we descend a new function $g$\n",
    "that is defined in terms of the\n",
    "gradients of our original function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "g(\\theta) = \\frac{1}{2}\\|\\nabla f(\\theta) \\|_2^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick application of the chain rule gives the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "\\nabla g(\\theta) &= \\nabla\\|\\nabla f(\\theta) \\|_2^2\\\\\n",
    "&= \\nabla\\nabla f(\\theta)\\cdot \\nabla f(\\theta)\\\\\n",
    "&= \\nabla^2f(\\theta)\\nabla f(\\theta)\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "leading to the update rule:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "\\theta^{t+1} &= \\theta^{t} - \\eta \\nabla g(\\theta)\\\\\n",
    "&= \\theta^{t} - \\eta\\nabla^2f(\\theta)\\nabla f(\\theta)\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare this to the Newton update:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "\\theta^{t+1} &= \\theta^{t} - \\gamma \\left(\\nabla^2f(\\theta)\\right)^{-1}\\nabla f(\\theta)\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the intuitive appeal of the former method,\n",
    "an analysis of its performance on polynomial functions\n",
    "would seem to indicate that it is a *bad idea*,\n",
    "because it converges more slowly when the step size is correctly chosen\n",
    "and diverges horribly if it is not.\n",
    "\n",
    "It is unclear, however, whether this carries over to other kinds of functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from crit_finder import train, evaluate\n",
    "from crit_finder.graphs import quadratics\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing with Identity Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The identity matrix quadratic form also makes a sanity check for the gradient norm minimization technique,\n",
    "since the Hessian is the identity,\n",
    "and so the updates from gradient norm minimization should exactly match those\n",
    "from gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "N = 2\n",
    "\n",
    "identity_matrix = np.eye(N).astype(np.float32)\n",
    "\n",
    "initial_values = quadratics.generate_initial_values(N)\n",
    "\n",
    "identity_quadratic_form = quadratics.make(identity_matrix, initial_values,\n",
    "                                                                 quadratics.DEFAULTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.3510263e-06, array([-0.00154637, -0.00055748], dtype=float32))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradmin_final_output, gradmin_final_parameters = quadratics.run_algorithm(identity_quadratic_form,\n",
    "                                                                             \"gradient_norm_min\", 50)\n",
    "gradmin_final_output, gradmin_final_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.3510263e-06, array([-0.00154637, -0.00055748], dtype=float32))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gd_final_output, gd_final_parameters = quadratics.run_algorithm(identity_quadratic_form,\n",
    "                                                                   \"gradient_descent\", 50)\n",
    "\n",
    "gd_final_output, gd_final_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all(np.equal(gradmin_final_parameters, gd_final_parameters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Symmetric Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We again first extend to the case of random symmetric matrices\n",
    "(for more on the specific random ensemble, see\n",
    "[the previous notebook](./01a - Critical Points of Quadratic Forms - Newton's Method.ipynb))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8901263e-09"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 5\n",
    "\n",
    "initial_values = quadratics.generate_initial_values(N)\n",
    "\n",
    "random_symmetric_matrix = quadratics.generate_symmetric(N)\n",
    "\n",
    "random_symmetric_quadratic_form = quadratics.make(random_symmetric_matrix, initial_values,\n",
    "                                                                 quadratics.DEFAULTS)\n",
    "\n",
    "_, values = quadratics.run_algorithm(random_symmetric_quadratic_form, \"gradient_norm_min\", 1500)\n",
    "\n",
    "np.linalg.norm(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite also using curvature information,\n",
    "gradient norm minimization takes *far* more steps to reach a given error than\n",
    "does Newton's method:\n",
    "while Newton can get to solutions with norm of order `1e-8` in two or three steps,\n",
    "gradient norm minimization takes thousands of steps even on small problems,\n",
    "with the number of steps increasing with problem size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Positive Definite Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The advantage of gradient norm minimization over Newton's method,\n",
    "however, is that it does not require a matrix inverse calculation.\n",
    "\n",
    "This has a computational benefit,\n",
    "since the matrix inversion step is complexity $O(n^{k})$\n",
    "for some $k$ in $(2, 3]$,\n",
    "depending on the algorithm,\n",
    "while all other computational steps are at most complexity $O(n^2\\log n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But even more crucially,\n",
    "avoiding the matrix inverse means that all of the issues regarding numerical non-invertibility\n",
    "that bedeviled the Newton's methods discussed in the last notebook\n",
    "are avoided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We again select random ill-conditioned and singular matrices\n",
    "according to the Wishart distribution,\n",
    "and we find that gradient norm minimization successfully finds a point with small gradient norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 500\n",
    "k = 100\n",
    "\n",
    "wishart_random_matrix = quadratics.generate_wishart(N, k)\n",
    "\n",
    "initial_values = quadratics.generate_initial_values(N)\n",
    "\n",
    "hyperparameters = {\"learning_rate\":0.1,\n",
    "            \"newton_rate\":1,\n",
    "            \"fudge_factor\":0.0,\n",
    "            \"inverse_method\":\"fudged\",\n",
    "            \"gradient_norm_min_rate\":0.001}\n",
    "\n",
    "\n",
    "wishart_quadratic_form = quadratics.make(wishart_random_matrix, initial_values,\n",
    "                                                                 hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.5502217e-07"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output, _ = quadratics.run_algorithm(wishart_quadratic_form, \"gradient_norm_min\", 1500)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output:\n",
      "\tinitial: 0.5760053396224976 \tfinal: 3.848661435768008e-06\n",
      "gradient norm:\n",
      "\tinitial: 2.7083492279052734 \tfinal: 0.00349063565954566\n"
     ]
    }
   ],
   "source": [
    "N = 500\n",
    "k = 100\n",
    "\n",
    "generate_full_rank_wishart = lambda N: quadratics.generate_wishart(N, k)\n",
    "\n",
    "evaluate.gradient_test(N,generate_full_rank_wishart,'gradient_norm_min', 1500, hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 500\n",
    "k = 500\n",
    "\n",
    "wishart_random_matrix = quadratics.generate_wishart(N, k)\n",
    "\n",
    "initial_values = quadratics.generate_initial_values(N)\n",
    "\n",
    "\n",
    "\n",
    "wishart_quadratic_form = quadratics.make(wishart_random_matrix, initial_values,\n",
    "                                                                 quadratics.DEFAULTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0014270995"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output, _ = quadratics.run_algorithm(wishart_quadratic_form, \"gradient_norm_min\", 1500)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output:\n",
      "\tinitial: 0.44226109981536865 \tfinal: 0.0013975112233310938\n",
      "gradient norm:\n",
      "\tinitial: 1.2912944555282593 \tfinal: 0.010902918875217438\n"
     ]
    }
   ],
   "source": [
    "N = 500\n",
    "k = 500\n",
    "\n",
    "generate_full_rank_wishart = lambda N: quadratics.generate_wishart(N, k)\n",
    "\n",
    "evaluate.gradient_test(N,generate_full_rank_wishart,'gradient_norm_min', 1500, quadratics.DEFAULTS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
