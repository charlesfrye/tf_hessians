{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second-Order Methods in TensorFlow - Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's implement a second-order method on a neural network.\n",
    "\n",
    "Again, we implement a version of Newton's method that's designed to find critical points, which need not be minima for non-convex functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NeuralNetwork = namedtuple(\"NeuralNetwork\", [\"graph\", \"graph_dictionary\", \"hyperparameter_dictionary\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is incomplete: right now, it's mostly just code copied from Part 2.\n",
    "\n",
    "Currently figuring out the best way to do the parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_neural_network(hyperparameter_dictionary):\n",
    "    \n",
    "    graph = tf.Graph()\n",
    "    \n",
    "    with graph.as_default():\n",
    "        \n",
    "        parameters = make_parameters(layer_sizes,\n",
    "                                     weight_initializer, bias_initializer,\n",
    "                                     hyperparameter_dictionary)\n",
    "        \n",
    "        #NETWORK CODE GOES HERE\n",
    "        \n",
    "        gradients = tf.gradients(cost, parameters, name=\"gradients\")\n",
    "        \n",
    "        hessian_matrix = tf.hessians(cost, parameters, name=\"hessian_matrix\")[0]\n",
    "        \n",
    "        fudging_vector = tf.constant([hyperparameters[\"fudge_factor\"]]*num_parameters,\n",
    "                                     dtype=tf.float64, name=\"fudging_vector\")\n",
    "        fudged_hessian = tf.add(tf.diag(fudging_vector),\n",
    "                                        hessian_matrix, name =\"fudged_hessian\")\n",
    "        inverse_hessian = tf.matrix_inverse(fudged_hessian, name=\"inverse_fudged_hessian\")\n",
    "\n",
    "        gradient_descent = tf.train.GradientDescentOptimizer(hyperparameters[\"learning_rate\"])\n",
    "        step_gradient_descent = gradient_descent.minimize(output)\n",
    "        \n",
    "        newton_base = tf.train.GradientDescentOptimizer(hyperparameters[\"newton_rate\"])\n",
    "        gd_grads_and_vars = newton_base.compute_gradients(output, inputs)\n",
    "        step_newton = add_step_newton(newton_base, gd_grads_and_vars, inverse_hessian)\n",
    "        \n",
    "        graph_dictionary = {\"inputs\": inputs,\n",
    "                            \"output\": output,\n",
    "                            \"cost\": cost,\n",
    "                            \"gradients\": gradients,\n",
    "                            \"hessian\": hessian_matrix,\n",
    "                            \"step_gradient_descent\": step_gradient_descent,\n",
    "                            \"step_newton\": step_newton\n",
    "                           }\n",
    "    \n",
    "    return NeuralNetwork(graph, graph_dictionary, hyperparameters)\n",
    "\n",
    "def add_step_newton(gradient_descent, gd_grads_and_vars, inverse_hessian):\n",
    "    gd_gradients, gd_variables = gd_grads_and_vars[0]\n",
    "    gd_gradient_vector = tf.expand_dims(gd_gradients, name=\"gradient_vector\", axis=1)\n",
    "\n",
    "    newton_gradient_vector = tf.matmul(inverse_hessian, gd_gradient_vector,\n",
    "                                           name=\"newton_gradient_vector\")\n",
    "    newton_gradients = tf.squeeze(newton_gradient_vector)\n",
    "      \n",
    "    newton_grads_and_vars = [(newton_gradients, gd_variables)]\n",
    "\n",
    "    step_newton = gradient_descent.apply_gradients(newton_grads_and_vars)\n",
    "    \n",
    "    return step_newton\n",
    "\n",
    "def make_parameters(layer_sizes, weight_initializer, bias_initializer, hyperparameter_dictionary):\n",
    "    raise(NotImplementedError)\n",
    "    layer_sizes.append(10) #readout_layer has 10 elements\n",
    "    \n",
    "    weight_matrix_shapes = zip(layer_sizes[1:], layer_sizes[0:])\n",
    "    bias_vector_shapes = layer_sizes\n",
    "    \n",
    "    for weight_matrix_shape, bias_vector_shape in zip(weight_matrix_shapes, bias_vector_shapes):\n",
    "        pass\n",
    "    \n",
    "    return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-nightly)",
   "language": "python",
   "name": "tf-nightly"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
