{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second-Order Methods in TensorFlow - Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'd like to actually implement a second-order method. Here, we implement a very silly version of Newton's method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimizing Quadratic Forms with Gradient Descent and Newton's Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "QuadraticFormMinimizer = namedtuple(\"QuadraticFormMinimizer\", [\"graph\", \"graph_dictionary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_quadratic_form_minimizer(matrix, initial_values, hyperparameters):\n",
    "    assert matrix.shape[0] == matrix.shape[1], \"only square matrices can be quadratic forms\"\n",
    "    assert matrix.shape[0] == len(initial_values), \"initial_values and matrix must match shape\"\n",
    "    \n",
    "    dimension = matrix.shape[0]\n",
    "    \n",
    "    graph = tf.Graph()\n",
    "    \n",
    "    with graph.as_default():\n",
    "        quadratic_form = tf.constant(matrix, name='quadratic_form')\n",
    "        \n",
    "        inputs = tf.get_variable(\"inputs\", shape=[dimension], dtype=tf.float64,\n",
    "                                initializer = tf.constant_initializer(initial_values))\n",
    "\n",
    "        input_vector = tf.reshape(inputs, (dimension, 1))\n",
    "    \n",
    "        output = tf.squeeze(tf.matmul(input_vector,\n",
    "                                  tf.matmul(quadratic_form, input_vector),\n",
    "                                      transpose_a=True,\n",
    "                                  name='output'),\n",
    "                        name='squeezed_output')\n",
    "        \n",
    "        gradients = tf.gradients(output, inputs, name=\"gradients\")\n",
    "        \n",
    "        hessian_matrix = tf.hessians(output, inputs, name=\"hessian_matrix\")[0]\n",
    "        \n",
    "        fudging_vector = tf.constant([hyperparameters[\"fudge_factor\"]]*N, dtype=tf.float64,\n",
    "                                     name=\"fudging_vector\")\n",
    "        fudged_hessian = tf.add(tf.diag(fudging_vector),\n",
    "                                        hessian_matrix, name =\"fudged_hessian\")\n",
    "        inverse_hessian = tf.matrix_inverse(fudged_hessian, name=\"inverse_fudged_hessian\")\n",
    "\n",
    "        gradient_descent = tf.train.GradientDescentOptimizer(hyperparameters[\"learning_rate\"])\n",
    "        step_gradient_descent = gradient_descent.minimize(output)\n",
    "        \n",
    "        newton_base = tf.train.GradientDescentOptimizer(hyperparameters[\"newton_rate\"])\n",
    "        gd_grads_and_vars = newton_base.compute_gradients(output, inputs)\n",
    "        step_newton = add_step_newton(newton_base, gd_grads_and_vars, inverse_hessian)\n",
    "        \n",
    "        graph_dictionary = {\"inputs\": inputs,\n",
    "                           \"output\": output,\n",
    "                           \"gradients\": gradients,\n",
    "                           \"hessian\": hessian_matrix,\n",
    "                            \"step_gradient_descent\": step_gradient_descent,\n",
    "                            \"step_newton\": step_newton\n",
    "                           }\n",
    "    \n",
    "    return QuadraticFormMinimizer(graph, graph_dictionary)\n",
    "\n",
    "def add_step_newton(gradient_descent, gd_grads_and_vars, inverse_hessian):\n",
    "    gd_gradients, gd_variables = gd_grads_and_vars[0]\n",
    "    gd_gradient_vector = tf.expand_dims(gd_gradients, name=\"gradient_vector\", axis=1)\n",
    "\n",
    "    newton_gradient_vector = tf.matmul(inverse_hessian, gd_gradient_vector,\n",
    "                                           name=\"newton_gradient_vector\")\n",
    "    newton_gradients = tf.squeeze(newton_gradient_vector)\n",
    "      \n",
    "    newton_grads_and_vars = [(newton_gradients, gd_variables)]\n",
    "\n",
    "    step_newton = gradient_descent.apply_gradients(newton_grads_and_vars)\n",
    "    \n",
    "    return step_newton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_minimizer(quadratic_form_minimizer, algorithm, num_steps):\n",
    "    graph, graph_dictionary = quadratic_form_minimizer\n",
    "    \n",
    "    with graph.as_default():\n",
    "        with tf.Session() as sess:\n",
    "            graph_dictionary[\"inputs\"].initializer.run()\n",
    "            for _ in range(num_steps):\n",
    "                sess.run(graph_dictionary[\"step_\"+algorithm])\n",
    "            output = sess.run(graph_dictionary[\"output\"])\n",
    "            values = graph_dictionary[\"inputs\"].eval()\n",
    "    \n",
    "    return output, values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing with Identity Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The identity matrix also makes a good test for the Newton method, since, if we set the `newton_rate`, or the learning rate of the Newton method, to exactly $1$, (and if we turn off Hessian-fudging, to be explained in a second) then we should get convergence in a single step.\n",
    "\n",
    "The value that minimizes the identity matrix is the same that minimizes any positive definite quadratic form: $\\mathbf{0}$, the zero vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "N = 2\n",
    "\n",
    "identity_matrix = np.eye(N)\n",
    "#initial_values = np.atleast_2d(np.sqrt([1/2,1/2])).T\n",
    "initial_values = np.random.standard_normal(size=N)\n",
    "\n",
    "identity_quadratic_form_minimizer = make_quadratic_form_minimizer(identity_matrix, initial_values,\n",
    "                                                                 {\"learning_rate\":0.1,\n",
    "                                                                 \"newton_rate\":1,\n",
    "                                                                 \"fudge_factor\":0.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output, final_parameters = run_minimizer(identity_quadratic_form_minimizer, \"newton\", 1)\n",
    "\n",
    "assert final_output == 0\n",
    "assert np.array_equal(final_parameters, [0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast, gradient descent takes more than one step to even approximately minimize this quadratic form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.60451602848147779, array([-0.31407713,  0.7112465 ]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_output, final_parameters = run_minimizer(identity_quadratic_form_minimizer, \"gradient_descent\", 1)\n",
    "\n",
    "final_output, final_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Positive Definite Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect that the Hessians of neural newtorks will behave approximately as if they were drawn from the Wishart distribution, which is generated by taking the outer product of a Gaussian random vector with itself.\n",
    "\n",
    "The spectra of these matrices should roughly follow the Mar√ßenko-Pastur distribution, which frequently produces eigenvalues that are close to or exactly $0$.\n",
    "\n",
    "This is a problem for the Newton method, which relies on inverting the Hessian -- a Hessian with some eigenvalues equal to $0$ is not invertible!\n",
    "\n",
    "We get around this with a *fudge factor*. Just adding a very small number to the diagonal of the Hessian adds that small number to all of its eigenvalues. The Newton updates are no longer exact, but at least they're not not-a-number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we generically see better performance for one step of the Newton method than of gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5\n",
    "\n",
    "self_outer_product = lambda x: x@x.T\n",
    "wishart_random_matrix = self_outer_product(np.random.standard_normal(size=(N,1)))\n",
    "\n",
    "initial_values = np.random.standard_normal(size=N)\n",
    "\n",
    "wishart_quadratic_form_minimizer = make_quadratic_form_minimizer(wishart_random_matrix, initial_values,\n",
    "                                                                 {\"learning_rate\":0.1,\n",
    "                                                                 \"newton_rate\":1,\n",
    "                                                                 \"fudge_factor\":10e-8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7.3880575715118251e-16,\n",
       " array([-1.23586491, -2.76591031,  0.40219102,  1.18296039, -0.53176368]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_minimizer(wishart_quadratic_form_minimizer, \"newton\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.00057802399664614798,\n",
       " array([-1.24228301, -2.76263253,  0.40258752,  1.18660558, -0.52548753]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_minimizer(wishart_quadratic_form_minimizer, \"gradient_descent\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Negative Definite Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Newton method is designed for convex problems. For non-convex problems, it does exactly the opposite of what it should: when the gradient says go down, it actually says go up! This causes it to be attaracted to maxima in this case. For a negative definite quadratic form, this maximum is at $0$.\n",
    "\n",
    "So if we try to run the Newton method in such a case, we should actually \"pessimize\", rather than optimize, the function. Gradient descent, even with just one step, will almost surely do better.\n",
    "\n",
    "For our work, this is a *good thing*, since it means we've found a place where the gradient is close to a $0$: a critical point that is not a minimum (or even a saddle!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5\n",
    "\n",
    "self_outer_product = lambda x: x@x.T\n",
    "negative_wishart_random_matrix = -1*self_outer_product(np.random.standard_normal(size=(N,1)))\n",
    "\n",
    "initial_values = np.random.standard_normal(size=N)\n",
    "\n",
    "negative_wishart_quadratic_form_minimizer = make_quadratic_form_minimizer(\n",
    "                                                    negative_wishart_random_matrix, initial_values,\n",
    "                                                                 {\"learning_rate\":0.1,\n",
    "                                                                 \"newton_rate\":1,\n",
    "                                                                 \"fudge_factor\":10e-8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-3.6472003033283916e-15,\n",
       " array([ 1.36625033, -1.22092162,  0.70182029,  1.471215  , -0.66823492]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_minimizer(negative_wishart_quadratic_form_minimizer, \"newton\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-35.439952592293167,\n",
       " array([ 1.72377876,  0.62870828,  1.97252841,  0.97166004, -3.08194262]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_minimizer(negative_wishart_quadratic_form_minimizer, \"gradient_descent\", 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-nightly)",
   "language": "python",
   "name": "tf-nightly"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
