{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second-Order Methods in TensorFlow - Part 3 - Sandbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's implement a second-order method on a neural network.\n",
    "\n",
    "Again, we implement a version of Newton's method that's designed to find critical points, which need not be minima for non-convex functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NeuralNetwork = namedtuple(\"NeuralNetwork\",\n",
    "                           [\"graph\", \"graph_dictionary\", \"hyperparameter_dictionary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tricky part here is specifying the parameters: in order to calculate a Hessian, we need the parameters to be inside the same variable, but TensorFlow is not designed with the expectation that all of our weights need to be initialized at once. In addition, this makes keeping abstraction barriers up more difficult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_neural_network(hyperparameter_dictionary):\n",
    "    \n",
    "    graph = tf.Graph()\n",
    "    \n",
    "    with graph.as_default():\n",
    "        \n",
    "        input_size = hyperparameter_dictionary[\"input_size\"]\n",
    "        output_size = hyperparameter_dictionary[\"output_size\"]\n",
    "        \n",
    "        num_parameters = calculate_num_parameters(hyperparameter_dictionary)\n",
    "        hyperparameter_dictionary[\"num_parameters\"] = num_parameters\n",
    "        parameters_placeholder = tf.placeholder(tf.float32, shape=[num_parameters],\n",
    "                                                name=\"initial_parameters\")\n",
    "        parameters_var = tf.Variable(initial_value=parameters_placeholder,\n",
    "                                         name=\"parameters_variable\")\n",
    "\n",
    "        weight_matrices, bias_vectors = make_weights_and_biases(parameters_var,\n",
    "                                                                hyperparameter_dictionary)\n",
    "        \n",
    "        input = tf.placeholder(tf.float32, shape=[None, input_size])\n",
    "        \n",
    "        network_output = build_by_layer(input, weight_matrices, bias_vectors,\n",
    "                                      hyperparameter_dictionary)\n",
    "        \n",
    "        labels = tf.placeholder(tf.float32, shape=[None, output_size])\n",
    "        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=network_output,\n",
    "                                                                     labels=labels))\n",
    "        \n",
    "        network_predictions = tf.nn.softmax(network_output, name=\"network_predictions\")\n",
    "        prediction_correct = tf.equal(tf.argmax(network_predictions,1), tf.argmax(labels,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(prediction_correct, tf.float32))\n",
    "        \n",
    "        with tf.variable_scope(\"grads_and_hess\"):\n",
    "\n",
    "            gradients = tf.gradients(cost, parameters_var, name=\"gradients\")\n",
    "\n",
    "            hessian_matrix = tf.squeeze(tf.hessians(cost, parameters_var, name=\"hessians_output\"),\n",
    "                                       name=\"hessian_matrix\")\n",
    "            \n",
    "            inverse_hessian = invert_hessian(hessian_matrix, num_parameters,\n",
    "                                            hyperparameter_dictionary)\n",
    "\n",
    "            gradient_descent = tf.train.GradientDescentOptimizer(hyperparameter_dictionary[\"learning_rate\"])\n",
    "            step_gradient_descent = gradient_descent.minimize(cost)\n",
    "\n",
    "            newton_base = tf.train.GradientDescentOptimizer(hyperparameter_dictionary[\"newton_rate\"])\n",
    "            gd_grads_and_vars = newton_base.compute_gradients(cost, parameters_var)\n",
    "            step_newton = add_step_newton(newton_base, gd_grads_and_vars, inverse_hessian)\n",
    "\n",
    "        graph_dictionary = {\"parameters_placeholder\": parameters_placeholder,\n",
    "                            \"parameters\": parameters_var,\n",
    "                            \"input\": input,\n",
    "                            \"weight_matrices\": weight_matrices,\n",
    "                            \"bias_vectors\": bias_vectors,\n",
    "                            \"labels\": labels,\n",
    "                            \"cost\": cost,\n",
    "                            \"accuracy\": accuracy,\n",
    "                            \"gradients\": gradients,\n",
    "                            \"hessian\": hessian_matrix,\n",
    "                            \"step_gradient_descent\": step_gradient_descent,\n",
    "                            \"step_newton\": step_newton\n",
    "                           }\n",
    "    \n",
    "    return NeuralNetwork(graph, graph_dictionary, hyperparameter_dictionary)\n",
    "\n",
    "def add_step_newton(gradient_descent, gd_grads_and_vars, inverse_hessian):\n",
    "    gd_gradients, gd_variables = gd_grads_and_vars[0]\n",
    "    gd_gradient_vector = tf.expand_dims(gd_gradients, name=\"gradient_vector\", axis=1)\n",
    "\n",
    "    newton_gradient_vector = tf.matmul(inverse_hessian, gd_gradient_vector,\n",
    "                                           name=\"newton_gradient_vector\")\n",
    "    newton_gradients = tf.squeeze(newton_gradient_vector)\n",
    "      \n",
    "    newton_grads_and_vars = [(newton_gradients, gd_variables)]\n",
    "\n",
    "    step_newton = gradient_descent.apply_gradients(newton_grads_and_vars)\n",
    "    \n",
    "    return step_newton\n",
    "\n",
    "def invert_hessian(hessian, num_parameters, hyperparameter_dictionary):\n",
    "    method = hyperparameter_dictionary[\"inverse_method\"]\n",
    "    \n",
    "    if method == \"fudged\":\n",
    "        fudging_vector = tf.constant([hyperparameter_dictionary[\"fudge_factor\"]]*num_parameters,\n",
    "                                         dtype=tf.float32, name=\"fudging_vector\")\n",
    "            \n",
    "        fudged_hessian = tf.add(tf.diag(fudging_vector),\n",
    "                                        hessian, name =\"fudged_hessian\")\n",
    "            \n",
    "        inverse_hessian = tf.matrix_inverse(fudged_hessian, name=\"inverse_hessian\")\n",
    "        \n",
    "    elif method == \"pseudo\":\n",
    "        \n",
    "        eigenvalues, eigenvectors = tf.self_adjoint_eig(tf.expand_dims(hessian, axis=0))\n",
    "        \n",
    "        threshold = hyperparameter_dictionary[\"minimum_eigenvalue_magnitude\"]\n",
    "        keep = tf.reduce_sum(tf.cast(tf.greater_equal(tf.abs(eigenvalues), threshold), tf.int32))\n",
    "        \n",
    "        truncated_eigenvalues = tf.squeeze(eigenvalues)[-keep:]\n",
    "        truncated_eigenvectors = tf.squeeze(eigenvectors)[:, -keep:]\n",
    "\n",
    "        inverse_hessian = tf.matmul(truncated_eigenvectors,\n",
    "                                    (1. / tf.expand_dims(truncated_eigenvalues, axis=0)) * truncated_eigenvectors,\n",
    "                                    transpose_b=True, name=\"inverse_hessian\")\n",
    "        \n",
    "    else:\n",
    "        raise NotImplementedError(\"no inverse hessian method for {0}\".format(method))\n",
    "        \n",
    "    return inverse_hessian\n",
    "\n",
    "def calculate_num_parameters(hyperparameter_dictionary):\n",
    "    layer_sizes = hyperparameter_dictionary[\"layer_sizes\"][:]\n",
    "    input_sizes = hyperparameter_dictionary[\"input_size\"]\n",
    "    output_size = hyperparameter_dictionary[\"output_size\"]\n",
    "    layer_sizes = [input_sizes] + layer_sizes + [output_size]\n",
    "    \n",
    "    num_weights = np.sum(np.multiply(layer_sizes[1:],layer_sizes[:-1]))\n",
    "    num_biases = np.sum(layer_sizes[1:])\n",
    "    \n",
    "    return num_weights+num_biases\n",
    "\n",
    "def make_weights_and_biases(parameters, hyperparameter_dictionary):\n",
    "    layer_sizes = hyperparameter_dictionary[\"layer_sizes\"][:]\n",
    "    input_sizes = hyperparameter_dictionary[\"input_size\"]\n",
    "    output_size = hyperparameter_dictionary[\"output_size\"]\n",
    "    layer_sizes = [input_sizes] + layer_sizes + [output_size]\n",
    "    \n",
    "    weight_matrices = make_weights(parameters, layer_sizes, hyperparameter_dictionary)\n",
    "    bias_vectors = make_biases(parameters, layer_sizes, hyperparameter_dictionary)\n",
    "    \n",
    "    return weight_matrices, bias_vectors\n",
    "\n",
    "def make_weights(parameters, layer_sizes, hyperparameter_dictionary):\n",
    "    weight_shapes = zip(layer_sizes[:-1], layer_sizes[1:])\n",
    "    starting_index = 0\n",
    "    weight_matrices = []\n",
    "    \n",
    "    with tf.variable_scope(\"weights\"):\n",
    "\n",
    "        for weight_shape in weight_shapes:\n",
    "            num_weights = weight_shape[0]*weight_shape[1]\n",
    "\n",
    "            weight_variables = tf.slice(parameters, [starting_index], [num_weights],\n",
    "                                        name=\"sliced\")\n",
    "            weight_matrix = tf.reshape(weight_variables, weight_shape,\n",
    "                                       name=\"reshaped\")\n",
    "\n",
    "            weight_matrices.append(weight_matrix)\n",
    "\n",
    "            starting_index += num_weights\n",
    "    \n",
    "    return weight_matrices\n",
    "\n",
    "def make_biases(parameters, layer_sizes, hyperparameter_dictionary):\n",
    "    bias_shapes = layer_sizes[1:]\n",
    "    total_biases = np.sum(bias_shapes)\n",
    "    total_weights = hyperparameter_dictionary[\"num_parameters\"]-total_biases\n",
    "    hyperparameter_dictionary[\"total_weights\"] = total_weights\n",
    "    hyperparameter_dictionary[\"total_biases\"] = total_biases\n",
    "    starting_index = total_weights-total_biases\n",
    "    bias_vectors = []\n",
    "    \n",
    "    with tf.variable_scope(\"biases\"):\n",
    "        \n",
    "        for bias_shape in bias_shapes:\n",
    "            num_biases = bias_shape\n",
    "            \n",
    "            bias_vector = tf.slice(parameters, [starting_index], [num_biases],\n",
    "                                     name=\"sliced\")\n",
    "\n",
    "            bias_vectors.append(bias_vector)\n",
    "            \n",
    "            starting_index += num_biases\n",
    "            \n",
    "    return bias_vectors\n",
    "\n",
    "def build_by_layer(input, weight_matrices, bias_vectors, hyperparameter_dictionary):\n",
    "    current_output = input\n",
    "    \n",
    "    for weight_matrix, bias_vector in zip(weight_matrices[:-1], bias_vectors[:-1]):\n",
    "        current_output = build_layer(current_output, weight_matrix, bias_vector,\n",
    "                             hyperparameter_dictionary)\n",
    "        \n",
    "    final_output = build_output_layer(current_output, weight_matrices[-1], bias_vectors[-1],\n",
    "                                      hyperparameter_dictionary)\n",
    "    \n",
    "    return final_output\n",
    "\n",
    "def build_layer(current_output, weight_matrix, bias_vector, hyperparameter_dictionary):\n",
    "    with tf.variable_scope(\"internal_layers\"):\n",
    "        nonlinearity = hyperparameter_dictionary[\"nonlinearity\"]\n",
    "        new_output = nonlinearity(tf.add(tf.matmul(current_output, weight_matrix), bias_vector))\n",
    "    return new_output\n",
    "\n",
    "def build_output_layer(current_output, weight_matrix, bias_vector, hyperparameter_dictionary):\n",
    "    with tf.variable_scope(\"output_layer\"):\n",
    "        final_output = tf.add(tf.matmul(current_output, weight_matrix), bias_vector)\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The biggest practical issue is handling the non-invertibility of the Hessian and related numerical issues in the Newton method. Seems like it requires fairly careful tuning of the step size, or else it can cause the performance to drop and/or the (raw) gradients to blow up.\n",
    "\n",
    "Need to look into trust region methods more closely for intuition about how to set this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tested with batch size 50k -- doesn't seem to be an error in estimating the Hessian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "01-24-2018 - Added pseudo-inverse based on Jesse's code. Gradient norm now goes down a bit and doesn't increase substantially, but it's not as small as I'd like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suggestion - if cost function is piecewise linear in the parameters, then gradient norm need not go to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fudged_hess_hyperparameters = {\"layer_sizes\":[10],\n",
    "                            \"nonlinearity\":tf.nn.sigmoid,\n",
    "                            \"input_size\":784,\n",
    "                            \"output_size\":10,\n",
    "                             \"learning_rate\":0.01,\n",
    "                             \"newton_rate\":1e-9,\n",
    "                             \"fudge_factor\":1e-4,\n",
    "                             \"inverse_method\": \"fudged\"\n",
    "                            }\n",
    "\n",
    "pseudo_inverse_hyperparameters = {\"layer_sizes\":[10],\n",
    "                            \"nonlinearity\":tf.nn.sigmoid,\n",
    "                            \"input_size\":784,\n",
    "                            \"output_size\":10,\n",
    "                             \"learning_rate\":0.01,\n",
    "                             \"newton_rate\":1e-5,\n",
    "                             \"minimum_eigenvalue_magnitude\":1e-7,\n",
    "                             \"inverse_method\": \"pseudo\"\n",
    "                            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time_stats = %prun -r network = make_neural_network(hyperparameter_dictionary)\n",
    "network = make_neural_network(pseudo_inverse_hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# time_stats_filename = \"time_stats_gpu.txt\"\n",
    "\n",
    "# with open(time_stats_filename, 'w') as time_stats.stream:\n",
    "#     time_stats.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = network.graph\n",
    "graph_dict = network.graph_dictionary\n",
    "hyperparameter_dictionary = network.hyperparameter_dictionary\n",
    "num_parameters = hyperparameter_dictionary[\"num_parameters\"]\n",
    "total_weights = hyperparameter_dictionary[\"total_weights\"]\n",
    "total_biases = hyperparameter_dictionary[\"total_biases\"]\n",
    "initialized_parameters = np.hstack([0.1*np.random.standard_normal(size=total_weights),\n",
    "                                  [0.1]*total_biases]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches_init = 5000\n",
    "num_newton_steps = 3\n",
    "\n",
    "gradient_descent_batch_size = 50\n",
    "newton_batch_size = 500\n",
    "\n",
    "print(hyperparameter_dictionary[\"inverse_method\"])\n",
    "print(hyperparameter_dictionary[\"newton_rate\"])\n",
    "\n",
    "with graph.as_default():\n",
    "    sess = tf.InteractiveSession()\n",
    "    input = graph_dict[\"input\"]\n",
    "    labels = graph_dict[\"labels\"]\n",
    "    initial_parameters = graph_dict[\"parameters_placeholder\"]\n",
    "    trained_parameters = graph_dict[\"parameters\"]\n",
    "    step_gradient_descent = graph_dict[\"step_gradient_descent\"]\n",
    "    step_newton = graph_dict[\"step_newton\"]\n",
    "    accuracy = graph_dict[\"accuracy\"]\n",
    "    gradient_op = graph_dict[\"gradients\"]\n",
    "    \n",
    "    initializer_feed_dict = {initial_parameters: initialized_parameters}\n",
    "    tf.global_variables_initializer().run(initializer_feed_dict)    \n",
    "    \n",
    "    for batch_idx in range(num_batches_init):\n",
    "        \n",
    "        if (batch_idx+1 == 1):\n",
    "            \n",
    "            batch_inputs, batch_labels = mnist.train.next_batch(gradient_descent_batch_size)\n",
    "            train_feed_dict = {input: batch_inputs,\n",
    "                       labels: batch_labels}\n",
    "            \n",
    "            acc = sess.run(accuracy, feed_dict=train_feed_dict)\n",
    "            gradients = sess.run(gradient_op, feed_dict=train_feed_dict)\n",
    "            gradient_norm = np.sqrt(np.mean(np.square(gradients)))\n",
    "            \n",
    "            print(\"init values\")\n",
    "            print(\"\\taccuracy: {0:.2f}\".format(acc))\n",
    "            print(\"\\tgrad_norm: {0:.10f}\".format(gradient_norm))\n",
    "            \n",
    "        batch_inputs, batch_labels = mnist.train.next_batch(gradient_descent_batch_size)\n",
    "        train_feed_dict = {input: batch_inputs,\n",
    "                       labels: batch_labels}\n",
    "        \n",
    "        sess.run(step_gradient_descent, feed_dict=train_feed_dict)\n",
    "        \n",
    "        acc = sess.run(accuracy, feed_dict=train_feed_dict)\n",
    "        \n",
    "        gradients = sess.run(gradient_op, feed_dict=train_feed_dict)\n",
    "        gradient_norm = np.sqrt(np.mean(np.square(gradients)))\n",
    "        \n",
    "        if (batch_idx+1 == 1) or ((batch_idx+1)%500 == 0):\n",
    "            print(\"grad step: {0}\".format(batch_idx+1))\n",
    "            print(\"\\taccuracy: {0:.2f}\".format(acc))\n",
    "            print(\"\\tgrad_norm: {0:.10f}\".format(gradient_norm))\n",
    "        \n",
    "    for batch_idx in range(num_newton_steps):\n",
    "        print(\"newton step: {0}\".format(batch_idx+1))\n",
    "        batch_inputs, batch_labels = mnist.train.next_batch(newton_batch_size)\n",
    "        train_feed_dict = {input: batch_inputs,\n",
    "                       labels: batch_labels}\n",
    "        \n",
    "        sess.run(step_newton, feed_dict=train_feed_dict)\n",
    "        \n",
    "        acc = sess.run(accuracy, feed_dict=train_feed_dict)\n",
    "        print(\"\\taccuracy: {0:.2f}\".format(acc))\n",
    "        gradients = sess.run(gradient_op, feed_dict=train_feed_dict)\n",
    "        gradient_norm = np.sqrt(np.mean(np.square(gradients)))\n",
    "        print(\"\\tgrad_norm: {0:.10f}\".format(gradient_norm))\n",
    "    \n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
