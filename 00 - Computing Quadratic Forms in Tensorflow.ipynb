{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Critical Points with TensorFlow\n",
    "## Part 0 - Working with Quadratic Forms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction: Why Critical Points?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *critical points* of a differentiable function that\n",
    "takes (possibly-multidimensional) real-valued input\n",
    "and produces a scalar real output are those points where\n",
    "the gradient of the function is equal to the $0$ vector.\n",
    "An example of such a function is\n",
    "the loss function of a neural network,\n",
    "either as a function of network inputs\n",
    "or as a function of network parameters.\n",
    "Recall that the gradient $\\nabla f$\n",
    "of a function $f$ at a point $\\theta$ is defined as the function that satisfies\n",
    "\n",
    "$$\n",
    "f(\\theta+\\epsilon) \\approx f(\\theta) + \\epsilon \\nabla f(\\theta)\n",
    "$$\n",
    "\n",
    "for sufficiently small $\\epsilon$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a generalization of the notion of critical points\n",
    "familiar from single-variable calculus,\n",
    "where a critical point is where the derivative is equal to $0$.\n",
    "In both cases, a critical point is a place where the\n",
    "*best linear approximation is constant*:\n",
    "\n",
    "$$\n",
    "f(\\theta+\\epsilon) \\approx f(\\theta) + \\epsilon 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Critical points are of interest because they are fixed points\n",
    "for the *gradient descent* optimization algorithm.\n",
    "In this algorithm, the value of $f(\\theta)$\n",
    "is minimized by, at each iteration,\n",
    "taking a small step in the direction that would\n",
    "most quickly minimize the linear approximation to $f$:\n",
    "\n",
    "$$\n",
    "\\theta^{t+1} = \\theta^t - \\epsilon\\nabla f(\\theta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the gradient is $0$, $\\theta^{t+1} = \\theta^t$,\n",
    "and the parameters don't change after an iteration.\n",
    "There is a long-standing debate whether neural networks reach critical points\n",
    "when they are being trained and, if so,\n",
    "to what kind of critical points they tend to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the linear approximation at every critical point is always a constant,\n",
    "the higher order approximations can be wildly different.\n",
    "The simplest way to differentiate critical points, then,\n",
    "is according to what the *best quadratic approximation* to the function looks like\n",
    "in the neighborhood of the critical point:\n",
    "\n",
    "$$\n",
    "f(\\theta + \\epsilon) \\approx f(\\theta) + \\epsilon\\nabla f(\\theta) + \\epsilon^\\intercal \\nabla^2 f(\\theta) \\epsilon\n",
    "$$\n",
    "\n",
    "Because the gradient is $0$ at a critical point,\n",
    "the best quadratic approximation to a function\n",
    "is parametrized by the matrix in the second term,\n",
    "its matrix of second partial derivatives,\n",
    "or Hessian,\n",
    "just as the best linear approximation is parametrized by its vector of first partial derivatives.\n",
    "We will be interested in the basis-independent properties of this matrix:\n",
    "is it singular? what is its spectrum? does it have an eigenvalue gap? is it poorly conditioned?\n",
    "and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it turns out, finding the critical points and calculating the curvature\n",
    "of a high-dimensional, non-polynomial function\n",
    "like a neural network loss function is quite difficult.\n",
    "These notebooks are a record of our attempts to develop\n",
    "and/or implement critical-point-discovery algorithms\n",
    "with an aim to be useful as tutorials.\n",
    "\n",
    "To develop our understanding of critical points and curvature,\n",
    "we first focus on problems for which\n",
    "*the best quadratic approximation is exact*.\n",
    "This will allow us to analytically derive the locations of critical points\n",
    "and the optimal values of hyperparameters\n",
    "and so verify that our algorithms are properly implemented.\n",
    "These functions are known as *quadratic forms*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quadratic Forms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quadratic form is a polynomial of degree two over an $n$-dimensional input. They are calculated as\n",
    "\n",
    "$$\n",
    "f(x) = \\frac{1}{2}x^\\intercal Q x\n",
    "$$\n",
    "\n",
    "where the factor of 1/2 is there to mostly simplify some later expressions\n",
    "(but much like the seemingly innocuous scaling factors in front of Fourier transforms,\n",
    "this scaling turns out to ensure certain favorable algebraic properties)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, I walk through the gradient and Hessian calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isn't a particularly interesting exercise, but it has allowed me to capture a number of bugs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from crit_finder import graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing with Identity Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest case, useful for working out basic bugs and flaws in reasoning, is the identity matrix, because  the quadratic form it defines is the squared $\\ell_2$ norm:\n",
    "\n",
    "$$\n",
    "\\frac{1}{2} x^\\intercal I x = \\frac{1}{2} x^\\intercal x = \\frac{1}{2}\\sum_i x_i \\cdot x_i = \\frac{1}{2}\\|x\\|_2^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 2\n",
    "\n",
    "identity_matrix = np.eye(N).astype(np.float32)\n",
    "input_vector = np.sqrt([1/2,1/2]).astype(np.float32)\n",
    "\n",
    "identity_quadratic_form_graph = graphs.make_quadratic_form(identity_matrix, input_vector,\n",
    "                                                                hyperparameters=graphs.DEFAULTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = graphs.get_result(\"output\", input_vector, identity_quadratic_form_graph)\n",
    "\n",
    "assert np.isclose(0.5*np.sum(np.square(input_vector)), out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient is just the input (and here the scaling by 1/2 is helpful):\n",
    "\n",
    "$$\n",
    "\\nabla_x \\frac{1}{2}x^\\intercal I x = \\frac{1}{2} \\nabla_x \\sum_i x_i\\cdot x_i = \\frac{1}{2}\\cdot2\\cdot I\\cdot x = x\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.70710677, 0.70710677], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient = np.squeeze(graphs.get_result(\"gradients\", input_vector, identity_quadratic_form_graph))\n",
    "\n",
    "gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(input_vector.T, gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Hessian matrix is therefore just $I$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hessian = np.squeeze(graphs.get_result(\"hessian\", input_vector, identity_quadratic_form_graph))\n",
    "\n",
    "hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(hessian, np.eye(N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, to really ensure that our code is correct, we should test on less-symmetric problems, e.g. matrices from the Gaussian ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generically, the gradient of $x^\\intercal Q x$ is\n",
    "\n",
    "$$\n",
    "\\nabla_x x^\\intercal Q x = (Q + Q^\\intercal)x\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which we can get from the definition of the derivative, following\n",
    "[this derivation from StackOverflow](https://math.stackexchange.com/questions/239207/hessian-matrix-of-a-quadratic-form).\n",
    "\n",
    "Our goal is to find $\\nabla_xf(x)$ such that\n",
    "\n",
    "$$\\begin{align}\n",
    "f(x+\\epsilon) &= f(x) + \\nabla_xf(x)\\epsilon + o(\\epsilon) \\\\\n",
    "\\end{align}$$\n",
    "\n",
    "as the norm of $\\epsilon$ goes to $0$.\n",
    "We expand the left-hand side,\n",
    "moving the factor of $\\frac{1}{2}$ over:\n",
    "\n",
    "$$\\begin{align}\n",
    "2\\cdot f(x+\\epsilon) &= (x+\\epsilon)^\\intercal Q (x+\\epsilon) \\\\\n",
    "&= x^\\intercal Q x\n",
    "+ x^\\intercal Q \\epsilon + \\epsilon^\\intercal Q x\n",
    "+ \\epsilon^\\intercal Q \\epsilon\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the rules for transposition and distribution, we can rewrite this as\n",
    "\n",
    "$$\\begin{align}\n",
    "2\\cdot f(x+\\epsilon)\n",
    "&= x^\\intercal Q x\n",
    "+ x^\\intercal Q \\epsilon + \\epsilon^\\intercal Q x\n",
    "+ \\epsilon^\\intercal Q \\epsilon \\\\\n",
    "&= x^\\intercal Q x\n",
    "+ (x^\\intercal Q \\epsilon)^\\intercal + \\epsilon^\\intercal Q x\n",
    "+ \\epsilon^\\intercal Q \\epsilon \\\\\n",
    "&= x^\\intercal Q x\n",
    "+ \\epsilon^\\intercal Q x + \\epsilon^\\intercal Q^\\intercal x\n",
    "+ \\epsilon^\\intercal Q \\epsilon \\\\\n",
    "&= x^\\intercal Q x\n",
    "+ \\epsilon^\\intercal (Q+Q^\\intercal) x\n",
    "+ \\epsilon^\\intercal Q \\epsilon \\\\\n",
    "f(x+\\epsilon) &= \\frac{1}{2} + x^\\intercal Q x\n",
    "+ \\frac{1}{2} \\epsilon^\\intercal (Q+Q^\\intercal) x\n",
    "+ \\frac{1}{2}\\epsilon^\\intercal Q \\epsilon \\\\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing this to the definition of the derivative above,\n",
    "we see that we have a term $f(x)$ and a term dominated by $\\epsilon$,\n",
    "leaving the middle term, sans $\\epsilon$, to be our derivative:\n",
    "\n",
    "$$\n",
    "\\nabla_x f(x) = \\frac{1}{2}(Q + Q^\\intercal)x\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, the Hessian is thus $\\frac{1}{2}(Q+Q^\\intercal)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 3\n",
    "\n",
    "random_matrix = np.random.standard_normal(size=(N,N)).astype(np.float32)\n",
    "\n",
    "input_vector = np.random.standard_normal(size=(N)).astype(np.float32)\n",
    "\n",
    "random_quadratic_form = graphs.make_quadratic_form(random_matrix, input_vector,\n",
    "                                                              graphs.DEFAULTS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24854209"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = graphs.get_result(\"output\", input_vector, random_quadratic_form)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.isclose(out, 0.5*input_vector.T.dot(random_matrix).dot(input_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.43652117,  0.6071776 , -0.21483205], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient = np.squeeze(graphs.get_result(\"gradients\", input_vector, random_quadratic_form))\n",
    "gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(gradient, 0.5*(random_matrix+random_matrix.T).dot(input_vector).squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.21663527, -1.2224835 ,  0.1898982 ],\n",
       "       [-1.2224835 , -1.4245507 ,  0.06303868],\n",
       "       [ 0.1898982 ,  0.06303868,  0.24287845]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_hessian = graphs.get_result(\"hessian\", input_vector, random_quadratic_form)\n",
    "tf_hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.21663527, -1.2224835 ,  0.1898982 ],\n",
       "       [-1.2224835 , -1.4245507 ,  0.06303868],\n",
       "       [ 0.1898982 ,  0.06303868,  0.24287845]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_hessian = 0.5*(random_matrix+random_matrix.T)\n",
    "true_hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(true_hessian, tf_hessian)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
